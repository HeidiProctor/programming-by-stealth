# PBS 101 of X â€” Introducing Version Control

In this first instalment of our second century we're starting a short pivot away from programming itself, and into one of the most important tools in a programmer's toolkit, *version control*. Version control isn't about version numbering conventions, alphas, betas, or anything like that, it's about capturing the full history of your software's existence so you can safely experiment and easily change your mind.

Programming is all about making choices â€” do I go with one approach or the other? Before you start writing your actual code one of the two options might seem simple, but once you start you often change your mind. Without version control backing out of a decision is messy and error prone, and changing your mind a second time soul-destroying. With version control you simply label the decision point, and start coding, if it works out, great, if not, you label your dead end, revert your code to how it was at the decision point, and start down the other path. If that proves even worse than your first path, you can simply label that new dead end, and jump back to your first path and continue where you left off.

Version control is extremely powerful, but with that power comes complexity. There are a lot of concepts to internalise, and because the tools are so flexible, an infinity of possible approaches you can take (and nerds can argue over). It takes time to find your perfect version control strategy, and the only way to get there is to get stuck in, to make mistakes, and to learn from them. It's OK to change your mind. It's OK to do things differently to your opinionated geek friend. Do what works for you, and just keep tweaking until you're happy.

We're going to focus on using version control to manage our code, but it can be used to manage anything. The Taming the Terminal book is managed through version control, as indeed are these very show notes you're reading now! If you can create it on a computer you can version it if you choose!

INSERT PLAYER HERE

## Two Totally Different Universes of Source Control

There is a massive dividing line through the middle of the universe of version control systems â€” there is the client-server model, and the peer-to-peer, or distributed, model. The client-server model has the advantage of intuitive simplicity, but it's fallen out of favour for many good reasons, and peer-to-peer is very much in the ascendancy these days.

### The Client-Server Approach

Presumably because it's so intuitive, this is how version control got started.

The model is built around the concept of a single central server holding the canonical truth about the versioned content. Creators can check out a given version of the content, make changes, and then submit those back to the server.

Whether you have one contributor or a million, there is never any doubt about what the official state of the code is, and, when two developers make conflicting changes, the rules for who has to re-do their work are trivial â€” first-come-first-served! If the server gets your version before Alice's, your commit will be accepted, Alice's will be rejected, and she'll have to figure out how to re-do her changes so they're compatible with yours.

For large institutions the need to run a server is not problematic, and the centralised control seems darn appealing, but things look very different outside of such institutions. Open Source communities are all about equal collaboration, rather than centralised management. Sure, one of the nerds can probably host the server, but who should be trusted to become that single point of control and failure? It's not a collaboration of equals if everything depends on one contributor hosting and managing a single server that holds the only canonical truth!

As for home users â€” needing to run a server just to keep track of your versions doesn't sound at all appealing!

Then you have the fact that you need access to the server to work effectively. You can't create a restore point without server access, so, you need network access each time you make a decision you might want to back out of. That's not a problem if you're a code monkey in a cubicle farm, but what if you want to make the most of some quiet time in a log cabin to really get some work done, or, as hard as it is to imagine in 2020, on a 12 hour direct flight from Dublin to LAX? (Been there, done that, got a lot of good coding done ðŸ™‚)

This is where both proprietary and open source version control started. In the open-source world the first client-server version control system to get traction was  CVS in 1990 (the *Concurrent Versioning System*), followed by the pinnacle of client-server open source version control, SVN (*Subversion*) first released a decade later in 2000.

Personally, I had the misfortune of needing to use CSV briefly before being allowed to switch to SVN. I actually really liked SVN, and since I was the kind of nerd who ran my own server, I stuck with it a lot longer than most, continuing to use it well into the twenty teens.

### The Peer-to-Peer AKA Distributed Model

When you check out code from a version control server you only get a snapshot of the project's history. Only the central server knows all. With a peer-to-peer model, every copy has the full history, so everyone knows everything.

In this distributed universe, there is no server. If there are three collaborators, all three have complete version histories, and all three are equal. They can offer updates to each other, but there is no onus on anyone to accept changes from anyone else. If everyone cooperates, everyone's versions stay in sync, if anyone gets fed up and leaves, their version diverges from the others.

Distributed version control systems don't enforce such utter chaos, an organisation can choose to host a copy that is considered canonical, but that's a human decision, and is in no way technically enforced. The agreed upon canonical copy is technologically no different to every other copy. What each developer has on their laptops is as complete as the officially blessed copy. Every contributor has everything, so they are free to go their own way when ever they wish.

Because every copy has everything, you don't need a server at all if you're a home user and you just want to code.

But, even if you do want to share. the fact that every copy has everything is a real game-changer. For a start, you don't need to be online to work!You can version away to your heart's content as you whizz across the Atlantic ocean in a cleverly-shaped metal tube, and then share all your work some time later, when ever it suits you. You can work away for a week or a month in your log cabin, and then share your work when you get home, or once a week when you go to down to get groceries and a sip of internet.

It's obvious why home users and the open source world embraced peer-to-peer version control, but why has it also taken off in the corporate world?

Well, when you combine corporate policies and a good VPN with a distributed model you can still exercise central control by managing the official master copy, but you can also facilitate off-line working by remote and traveling employees. When they clone the official repository they can work offline for as long as they need to, and when they have internet access they can VPN to the office and synchronise their full copy with the master copy.

While there were distributed open source version control systems before the mid-2000s, that seems to be when the idea really took hold. In 2005 alone three potential heirs to SVN's crown were released, *GNU Bazaar*, *Mercurial*, and *Git*. The GNU offering never quite got traction, but Mercurial really started to gather steam. The hands-down winner though is undoubtedly Git, because it was built specifically to manage the Linux kernel. Can there be a more emblematic open source project than that?

To say Git hit the ground running is an understatement. Linux Torvalds himself started the code on April 3 2005, announced it to the public on April 6th, and the Git project became self-hosted (i.e. Git was versioned in Git) from the very next day, April 7th. Four days to go from idea to functional enough to version its own code!

The next big milestone for Git was the release of the Linux 2.6.12 Kernel on June 6th, the first version of the kernel to be versioned using Git. Torvalds handed the project to Junio Hamano in July 2005, and Hamano has been the chief maintainer ever since.

Because Git proved itself capable of managing the most iconic open source project of all, it soon went viral, and it now absolutely dominates the open source world.

As you've probably guessed by now, we'll be learning the concepts of distributed source control using Git.

## Out Git Journey

